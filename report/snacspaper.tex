%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Social Network Analysis for Computer Scientists
% Course paper template (modified version of ACM Proceedings template)
% Frank W. Takes (ftakes@liacs.nl)
% http://liacs.leidenuniv.nl/~takesfw/SNACS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\usepackage{url}
\usepackage{color}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}

\hypersetup{
	bookmarks=true,         % show bookmarks bar?
	unicode=false,          % non-Latin characters in Acrobat’s bookmarks
	pdftoolbar=true,        % show Acrobat’s toolbar?
	pdfmenubar=true,        % show Acrobat’s menu?
	pdffitwindow=false,     % window fit to page when opened
	pdfstartview={FitH},    % fits the width of the page to the window
	pdftitle={My title},    % title
	pdfauthor={Author},     % author
	pdfsubject={Subject},   % subject of the document
	pdfcreator={Creator},   % creator of the document
	pdfproducer={Producer}, % producer of the document
	pdfkeywords={keyword1, key2, key3}, % list of keywords
	pdfnewwindow=true,      % links in new PDF window
	colorlinks=false,       % false: boxed links; true: colored links
	linkcolor=red,          % color of internal links (change box color with linkbordercolor)
	citecolor=green,        % color of links to bibliography
	filecolor=magenta,      % color of file links
	urlcolor=cyan           % color of external links
}

\begin{document}

\title{Course Project Title}
\subtitle{Social Network Analysis for Computer Scientists --- Course Project Paper}

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
	Name One\\
	\affaddr{LIACS, Leiden University}\\
	\email{sXXXXX@umail.leidenuniv.nl}
% 2nd. author
\alignauthor
	Name Two\\
	\affaddr{LIACS, Leiden University}\\
	\email{sXXXXX@umail.leidenuniv.nl}
}

\permission{This paper is the result of a student course project, and is based on methods and techniques suggested in \cite{bowman:reasoning, clark:pct, braams:babel, herlihy:methodology}.  % NOTE for SNACS: replace these citations with the papers you studied!
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice on the first page. }
\conferenceinfo{SNACS '17}{Social Network Analysis for Computer Scientists, Master CS, Leiden University (\url{liacs.leidenuniv.nl/~takesfw/SNACS}).}

\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Abstract.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{Theory}

\keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}
%In this paper we are going to try to share our own contribution and important factors for link prediction in all kinds of networks. 

One of the main ideas behind link prediction is that it offers several ways in which one can study networks of any kind in which data scientists, software engineers and researchers could greatly benefit from. Social networks are a popular way to model the interactions among people in a group or community. Their connections can be visualized as a graph, where a vertex corresponds to a person in a group and an edge represents some form of association between the corresponding persons.

Organizations, such as Amazon, can extract information based on raw data in order to predict what the customer might actually buy or find interesting. Another example would be the professional social-media platform, Linked-in.  Based on incoming data they would be able to, for instance predict your next connection; link back jobs which are relevant to you; link contents/articles based on your connections, interests, and things you read on the web. Similarly, security corporate companies could more precisely focus their efforts based on probable relationships in malicious networks that have heretofore gone unobserved, and researchers can easily adapt link prediction methods to identify links that are surprising given their surrounding network, or links that may not exist at all \cite{Lichtenwalter:2010:NPM:1835804.1835837}. 

That being said, links or associations are usually based on someone's behavior and interests. In social networks however, objects are dynamic given that the number of edges and nodes are changing continuously. Getting a hold of those dynamics [number of nodes and edges] is one of the fundamental problems in social network analysis. One of the essential question(s) which we are going to cover in this paper is, given a network at time $t$, accurately predict the links that are going to form at a future time $t'$. This is known as the link prediction problem.

In this paper, we approach the problem of link prediction using supervised learning \cite{Fire2013}, \cite{Hasan06linkprediction}, \cite{Lichtenwalter:2010:NPM:1835804.1835837}, \cite{Wang:2007:LPM:1441428.1442084}. Unsupervised techniques have also been used to solve the problem of link prediction. The main difference between the two is that in the unsupervised case, we calculate metrics that individually provide predictions of whether a link will form or not. Supervised learning employs a more sophisticated framework where each of these metrics could be considered as an input to a machine learning model.

%We also learned other papers are in existence in regards of supervised learning which cover the matters of link prediction problem such as in [2] and [17] in which the authors merely cover the results of the test set(s) being unbalanced filled with negative instances. Our main contributing will mainly focus on this paper [1] and the methods and techniques in which we can, additionally, contribute to this paper. 

%In link prediction, we need to transform networks in actual data sets to apply machine learning models. Due to the way of constructing those data sets, there is a great imbalance between negat Data sets constructed from networks Due to the high class imbalance in data sets constructed from networks 


This paper is heavily based on methods and techniques described in \cite{Hasan06linkprediction} and \cite{Lichtenwalter:2010:NPM:1835804.1835837}. Both papers consider collaboration networks of co-authors on which several machine learning models are applied in order to predict future links (collaborations) between co-authors. Under sampling is used in both papers in order to solve the problem of class imbalance and decrease the size of the training sets. Additionally, in \cite{Lichtenwalter:2010:NPM:1835804.1835837}, the training set is constructed by considering the $n$-neighborhood of each node. Moreover, it is proposed to use a specific type of decision tree, called Hellinger Distance Decision Tree (HDDT) \cite{Cieslak2008}, \cite{Cieslak2012}, applied in the original distribution of the data, that is without employing under sampling. Those trees use the Hellinger distance as splitting criterion, which is skew insensitive \cite{Cieslak2012}, and therefore is very well suited for classification with imbalanced data. Although this recommendation is provided, the authors do not present results of this method. The contributions of this paper are as follows:

\begin{enumerate}
	\item Although there is an in-depth comparison of Hellinger Distance Decision Trees and regular Random Trees (RT) (that use entropy as splitting criterion) in \cite{Cieslak2012}, to the best of our knowledge, Hellinger Distance Decision Trees have not been applied to solve the problem of link prediction, in which the class imbalance could be as high as 1:20000
	\item We compare two methods based on \cite{Hasan06linkprediction} and \cite{Lichtenwalter:2010:NPM:1835804.1835837}. The first method consists of under sampling the data and fitting RT and HDDT, whereas in the second method we fit those models to the original distribution of our data, that is, without under sampling.
	\item Regarding the evaluation of results, apart from the ROC/AUC metric which is proposed in \cite{Lichtenwalter:2010:NPM:1835804.1835837}, we also use the Precision-Recall curve as suggested in \cite{saito2015precision}, which is better suited for imbalanced data sets. 
\end{enumerate}
 
%Our part of the research has the following contributions:
%1- We will end-up explaining different concept of algorithms and machine learning tack ticks in order to perform (link-predict) the next link between to objects. 
%2- We will spend some time experimenting with datasets to learn more about algorithms and perhaps finding differences or comparison between number of algorithms .   

The paper is structured as follows. In Section 2, we formalize the link prediction problem. Specifically, we discuss the transformation of a network to an ordinary data set, and the class imbalance problem. Related work is provided in Section 3. In Section 4, we describe in detail the methods compared in this paper. Section 5 describes the data sets in which the experiments and results of Section 6 are based on. Finally, we conclude with Section 6.  

%In the following sections we will be guiding you through number of problem sets; working with datasets; talking about finding similar or relevant papers; explaining or learning about different algorithms; Experiment and sharing results; sharing our thoughts and finishing things up with some discussions and conclusions  

\section{Problem Statement} 

Consider at a particular time $t$, an undirected network $G = \langle V,E \rangle$, where $V$ is the set of nodes and $E$ is the set of edges. The goal is to predict the edges that will form in the network at time $t' > t$. As an example, consider a collaboration network of co-authors. Two authors $v$ and $w$ are linked when they have published at least one research paper together. The goal is to predict future connections between co-authors that have not yet collaborated.

In this paper, we approach the problem of link prediction using supervised learning. In this framework, in order to build models with predictive ability, one needs to construct two sets, mainly a \textit{train} and a \textit{test} set. Then, the model learns features and patters from the train set, and it is evaluated on the test set. These two sets form together the \textit{data set}. This set is of the form $(\vec{x},y)$, where $\vec{x}$ indicates the inputs to the model, and $y$ indicates the label, that is whether or not a link is present between two nodes. The model learns from the instances $(\vec{x},y)$ of the train set. In the evaluation phase, we feed the instances $\vec{x}$ of the test set to the trained model, and the output is the predicted label, i.e our prediction of whether or not a link will form between two nodes.

 We face two problems here. First, how to construct the train and test set, and secondly, what are $\vec{x}$, $y$ in our model.

\subsection{Constructing Data Sets}
The process of constructing the train and test set is described in detail in \cite{Hasan06linkprediction}, \cite{Lichtenwalter:2010:NPM:1835804.1835837}. In order to transform the network to a data set, we first choose two adjacent periods, the \textit{train} and \textit{test period}. In the train period, denoted by $[0,t_x]$, we have pairs of authors that are connected, that is, they have written at least one research paper together. The same holds for the test period $[t_{x+1},t_y]$. We construct a new network $G_x = \langle V_x, E_x \rangle$ consisted of all pair of authors that do not share a connection in the train period. We then examine whether or not they co-authored a paper in the test period. If they did, we assign to the pair a positive label, and otherwise a negative label. The data set contains $|V_x| \choose 2$ $-E_x$ instances, since we do not account for self-loops, and in undirected networks each pair is recorded once. 

Selecting an appropriate set of features is one of the most important part in supervised learning algorithms \cite{Hasan06linkprediction}. The inputs to the model are selected by calculating topology-based and node-based metrics from $G_x$ that will serve as attributes to the data set. Such metrics include, but not limited to, \textit{common neighbors}, \textit{Jaccard coefficient}, \textit{Adamic Adar}, \textit{preferential attachment}, \textit{Katz} measure, \textit{PropFlow}, \textit{SimRank}, and many others. All these features can be used individually to provide naive predictions. This is sometimes called the unsupervised framework of link prediction. In the supervised case, we use state-of-the-art predictive models such as Neural Networks, Support Vector Machines, Decision Trees, and Random Forests that treat these individual metrics as inputs. The output or prediction of the model, is whether or not a link will form between two specific nodes. 

That being said, we end up with a data set in the standard format $(\vec{x},y)$, where $\vec{x}$ are metrics extracted from $G_x$ and $y$ indicates the label, that is whether or not a pair of nodes is connected or not. 

\subsection{Class Imbalance and Size}
\label{Sect.2b}
Although we have constructed the data set, there are two main problems. First, the size of the data set is very large which can make the training process infeasible. For example, consider the relatively small in size \texttt{ca-CondMat} network \cite{Leskovec:2007:GED:1217299.1217301}, consisted of 23133 nodes and 93497 edges. Defining the train period to be two-thirds of the networks' edges, and assuming that all node appear in $V_x$, the resulting data set would be of size $23133 \choose 2$ $ - 62631$, which yields more than 267 million instances. Additionally, the distribution of the classes is highly imbalanced. That is, the amount of instances belonging to the negative class is overwhelmingly greater than the instances of  the positive class. Those two issues make the training process difficult or even impossible. There mainly exist two approaches to solve these problems.

One method consists of under sampling the data set to balance in order to create classes of almost the same size. In this way, the size of the training set decreases, and at the same time we obtain balanced classes in which we can fit ordinary predictive models such as, Decision Trees, Support Vector Machines, Neural Networks etc. Note that many of these models perform poorly in highly imbalanced data. 	 

A second method is proposed in \cite{Lichtenwalter:2010:NPM:1835804.1835837}. Specifically, it is suggested to treat each neighborhood as a separate problem. We define the $n$-neighborhood of a node $v$ as the set of nodes that are 0 to $n$ steps away from $v$. Instead of considering all pairs of nodes to enter the data set, we choose for each node, and for a specified $n$, only those that belong to the $n$-neighborhood of the node. In this way, the size of the data set decreases greatly. Furthermore, it is proposed to implement a particular type of decision trees, called Hellinger trees  \cite{Cieslak2008}, \cite{Cieslak2012}, without altering the distribution of the data, that is without under sampling. Hellinger trees use the Hellinger distance as splitting criterion, which is skew insensitive \cite{Cieslak2012}.



%A solution is proposed in \cite{Lichtenwalter:2010:NPM:1835804.1835837}. Specifically, it is suggested to treat each neighborhood as a separate problem. We define the $n$-neighborhood of a node $v$ as the set of nodes that are 0 to $n$ steps away from $v$. Instead of considering all pairs of nodes to enter the data set, we choose for each node, and for a specified $n$, only those that belong to the $n$-neighborhood of the node. In this way, the size of the data set decreases greatly.

%Although the aforementioned solution mitigates the issue of class imbalance, the problem still persists. There are two approaches to ensure that the predictive model provides sufficiently good results. Since ordinary models such as, for example, Decision Trees perform poorly in highly unbalanced data, the first method consists of under sampling the data set to balance in order to create classes of almost the same size. The second approach \cite{Lichtenwalter:2010:NPM:1835804.1835837}, involves training a particular type of decision trees, called Hellinger trees \cite{Cieslak2012}, \cite{Cieslak2008}, without altering the distribution of the data. The Hellinger distance, which is skew insensitive is used as the splitting criterion \cite{Cieslak2012} in this kind of decision trees.

\section{Related Work}

The link prediction problem was formalized in \cite{Liben-Nowell:2003:LPP:956863.956972}, where there is an extensive analysis of different baseline predictors, applied to co-authorship networks. Nevertheless, state-of-the-art supervised learning models are not studied in this paper. Furthermore, co-authorship networks have been analyzed in \cite{Hasan06linkprediction}, \cite{Pavlov:2007:FEL:2889513.2889517}, and \cite{de2011supervised}, but this time several machine learning algorithms such as Decision Trees, SVM, Neural Networks, and K-Nearest Neighbors are used for prediction. In contrast to \cite{Hasan06linkprediction}, where static, unweighted networks are used, \cite{Pavlov:2007:FEL:2889513.2889517} takes into account the evolution of the network, and the authors of \cite{de2011supervised} analyze a weighted network. Several recommendations and suggestions are presented in \cite{Lichtenwalter:2010:NPM:1835804.1835837}, regarding how to perform link prediction, and how to approach inherent issues such as dealing with class imbalance and huge train sets. 

In the domain of counter-terrorism, link predictions has been studied in \cite{dombroski2003estimating}, \cite{Fire2013}. In the latter paper, several networks are constructed by removing nodes from the original graph, called visible networks. A set of features is calculated from these networks in order to predict links of the original one, using supervised learning models. A more probabilistic approach is taken in \cite{doppa:nips09-wkshp}, where chance-constrain programs are used, and in \cite{NIPS2003_2465}, in which the framework of Relational Markov Network is utilized. Finally, \cite{DBLP:journals/corr/WangXWZ14}, and \cite{Hasan2011} are extensive surveys regarding the different metrics, classification methods, and approaches that have been used for link prediction. 

%The main purpose of this paper is to compare the methods applied in \cite{Hasan06linkprediction}, and \cite{Lichtenwalter:2010:NPM:1835804.1835837}. Specifically, in \cite{Hasan06linkprediction}, under sampling of the data is used in order to decrease the size of the training set and apply several machine learning algorithms. On the other hand, in \cite{Lichtenwalter:2010:NPM:1835804.1835837} the problem is approached by considering each neighborhood as a separate problem, as we discussed in the Section 2.2. In the latter paper, both the methods of under sampling to balance, and the use of Hellinger trees are discussed. Nevertheless, the authors chose to provide results only for the under sampling method. Therefore, the purpose is to fill this gap by first, comparing the methods in the two papers, and additionally apply Hellinger trees in the original distribution of the data, without the use of under sampling. Details regarding the two approaches and the differences between them are provided in the next section.

\section{Suggested Approaches}

In this section, we describe the approaches that have been taken in \cite{Hasan06linkprediction} and \cite{Lichtenwalter:2010:NPM:1835804.1835837} and we discuss the differences between them. Our approach, which is based on both papers, is applied in 5 social networks with the aim to compare DT which are applied in \cite{Hasan06linkprediction} and HDDT which are proposed in \cite{Lichtenwalter:2010:NPM:1835804.1835837}, but the authors have not provided results for the latter method.


The authors in \cite{Hasan06linkprediction} use two undirected, unweighted collaboration networks of co-authors, mainly the BIOBASE \footnote{\url{http://www.elsevier.com}} and DBLP \footnote{\url{http://dblp.uni-trier.de/xml/}} networks. An edge between two authors/nodes exists in case they have written at least on research paper together. Both networks contain timestamps of the years of collaborations. For each network, the authors choose a corresponding train and test period that span one or more years. For example, in the first network, the train period is consisted of 5 years from 1998 to 2002, whereas the test period comprises the year 2003. The data set constructed from this network contains each combination of co-authors that do not exist in the train period, that is, each pair that could potentially have a connection in the future. For each of these combinations, it is examined whether or not there is a connection in the test period. If a connection exists, a positive label is assigned to the pair, otherwise a negative label. Thus, the problem of link prediction has been framed as a binary classification problem in which any known machine learning classification model can be applied in order to distinguish between the positive and negative classes.  

For each resulting pair of nodes, several topological and domain specific features are calculated. The former kind of features include the \textit{shortest dictance} and \textit{clustering index}, whereas the domain specific features include the sum of papers the two authors share, the sum of the count of keywords in the papers of each author, the number of common keywords two authors have used etc. 

One of the main characteristics of the link prediction problem is the great class imbalance between positive and negative classes. This is due to the fact that the number of potential links that could form is enormously larger than the links that do actually form \cite{Lichtenwalter:2010:NPM:1835804.1835837}. Additionally, the resulting data set has a very large size that could make the training process of a classifier even infeasible. To see this, refer to the example described in Section \ref{Sect.2b}. In \cite{Hasan06linkprediction} these problems are solved by under sampling the data to balance. Under sampling is a widely used technique in order to cope with imbalanced classes in which we only keep a representative sample of the large class that is more or less equal to the size of the small class. Specific ratios of imbalance can also be specified in the sampling process. The resulting data set is comprised from the sample of the large class and all instances of the small class, resulting into a balanced data set. 

After creating a balanced data set, the authors train several machine learning models such as Decision Trees (with and without Bagging), Support Vector Machines, Naive Bayes, Neural Networks and others. They found that Bagging and Support Vector Machines provided the best performances when compared to different metrics such as accuracy, precision, recall, F-value and squared error. The models are evaluated using 5-fold cross validation.

The second paper in which this work is heavily based, is that of Lichtenwalter et al. \cite{Lichtenwalter:2010:NPM:1835804.1835837}. The techniques described in this paper are applied in two networks, one weighted, undirected co-authorship network of 19464 edges of condensed matter physics collaborations from 1995 to 2000 called \texttt{condmat}, and one directed weighted network of 712 million cellular phone calls, called \textit{phone}. In principle, the construction of train and test sets is the same, that is, topological and node attribute measures are extracted from the network constructed in the train period, and the labeling is being done using the network of the test period.

The authors of the paper treat the problem of class imbalance in great detail. They suggest to construct the data set in the following way. Instead of considering all possible combinations of pairs of nodes that do not exist in the train period, we construct the data set only from those combinations that are not part of the train period, but also contained in the neighborhood of $n$ size. Specifically, for each node $v$, we calculate its $n$-size neighborhood. We then compute the metrics between $v$ and the nodes of this neighborhood, by excluding pairs that already belong to the train period. The idea behind this is that, it is more possible for links to form in the future between nodes that are closely together, than with nodes that are far apart. Thus, reducing the problem to each neighborhood offers two advantages. Firstly, the imbalance between classes is decreased, and secondly given that specific metrics such as \textit{Adamic Adar} and \textit{Rooted PageRank} are expensive to compute, we avoid calculating those metrics for pairs of nodes that are not highly likely to connect in the future. The value of $n$ is chosen by the user. Selecting higher values guarantee that we do not lose connections that actually do form, but on the other hand the instances of the negative class increase dramatically, along with the computation time of the metrics.

Although choosing smaller values for $n$ mitigates the problem of class imbalance, the distributions' skew is still heavy, with the negative class dominating the positive. As an example, for the \texttt{condmat} network in \cite{Lichtenwalter:2010:NPM:1835804.1835837}, values of $n=2$ and $n=4$ result in classes of 179:1 and 6247:1, respectively. For the larger \texttt{phone} network, the imbalance is even stronger. For the aforementioned values of $n$, the ratios are 131:1 and 32880:1, respectively.

Apart from the aforementioned suggestion of treating the problem locally for each neighborhood, the authors in \cite{Lichtenwalter:2010:NPM:1835804.1835837} also provide their insights and suggestions regarding sampling techniques. One technique that can be used is called SMOTE \cite{chawla2002smote}, in which the minority class is over sampled in order to balance the data set. Although that could work in a network such as \texttt{condmat}, it will certainly not for \texttt{phone}, due to its size \cite{Lichtenwalter:2010:NPM:1835804.1835837}. The problem is that by using over sampling techniques we increase the size of the train set which we wanted to reduce in the first place. Regarding under sampling, the authors experiment with different sampling ratios, that is, they explicitly under sample the data to different ratios than 1:1, and then they evaluate a C4.5 Decision Tree \cite{quinlan2014c4} by reporting the AUC measure of the respective ROC curve. The idea is that by keeping a ratio of imbalance in the under sampled data set, we retain information from the network, which we might lose when under sampling to 1:1 ratio. Although this is a valid approach, the authors evaluate the classifier trained on imbalanced data using the AUC of the ROC curve, which might be misleading, since the ROC curve is insensitive to skew distribution \cite{saito2015precision}. 

Additionally, the High Performance Link Prediction (HPLP) framework is presented in \cite{Lichtenwalter:2010:NPM:1835804.1835837}. This framework comprises several topological metrics that can be computed for every network such as \textit{In/Out Degree}, \textit{Common Neighbors}, \textit{Maximum Flow}, \textit{Adamic Adar}, \textit{Jaccard Coefficient}, and others. Note that this is different from the work of Hasan et al. \cite{Hasan06linkprediction} where most of the selected features are domain specific. Hence, HPLP serves as a general framework for link prediction. Additionally, a new unsupervised prediction method is presented called \texttt{PropFlow}. It is a predictor based on random walk that starts from node $v_i$ and ends at a node $v_j$ in $l$ or fewer steps, using link weights as transition probabilities \cite{Lichtenwalter:2010:NPM:1835804.1835837}. A score $s_{ij}$ is produced for each pair of nodes $v_i$ and $v_j$ that serve as an estimation of the likelihood that the two nodes will be connected in the future. \texttt{PropFlow} is also included in HPLP.

Finally, after under sampling to balance both data sets, the authors apply Random Forests with Bagging, using the features proposed in the HPLP framework, and for different values of $n$. They found that with increasing values of $n$ there is a deterioration in performance, in both networks. The value of $n=2$ provided the best performance.

In this paper, we compare Decision Trees and Hellinger Distance Decision Trees with bagging as applied and proposed in \cite{Hasan06linkprediction} and \cite{Lichtenwalter:2010:NPM:1835804.1835837}, respectively, for the case of under sampling to balance (ratio 1:1), and for an imbalanced ratio 1:6, for 5 social networks. To the best of our knowledge, Hellinger Distance Decision Trees have not been applied to solve the link prediction problem. We are interested to examine whether or not HDDT provide sufficiently good results after constructing a data set from a social network and under sampling to a ratio of 1:6.

\section{Data Sets and Software Tools}

Almost all link prediction works need to verify their methods on the collected datasets. The datasets are important for fairly reproducing and comparing different link prediction methods. Constructing and collecting the datasets is a time-consuming and labor-intensive work. That said, not all the datasets are publicly available to use and some of them are incomplete. Since the process of transforming the network to a data set requires to create two non-overlapping periods that simulate the formation of links between nodes from one period to the other, it is essential to consider networks that contain timestamps. We are interested in both directed and undirected networks. Nevertheless, we convert the former type to the latter. In all of our experiments, we consider unweighted networks.

%As mentioned in the sub-section Constructing Data Sets the dataset needs splitting into training sets and testing sets and to link-predict correctly our focus lies specifically on datasets with timestamps attached to each set of points this way we can easily check if a set exists in training set and test set given the dates of, for instances two authors previously worked together and might work together in the near future. 

%During scouting on the web for datasets we came across several useful datasets to which we think we can work with and produce some results with it. 

We use two networks from that come from the KONECT \footnote{\url{http://konect.uni-koblenz.de/networks/}} database such as the \texttt{UC Irvine messages} \footnote{\url{http://konect.uni-koblenz.de/networks/opsahl-ucsocial}} \cite{opsahl2009clustering}, a directed network containing sent messages between the users of an on-line community of students from the University of California, Irvine. Next, \texttt{Digg} \footnote{\url{http://konect.uni-koblenz.de/networks/munmun_digg_reply}}\cite{de2009social} is a reply directed network of the social news website Digg where each node in the network is a user of the website, and each directed edge denotes that a user replied to another user. Furthermore, we use the \texttt{Dublin Contacts} \footnote{\url{http://networkrepository.com/ia-contacts-dublin.php}} \cite{nr} undirected network in which a node represents a human and an edge corresponds to a contact in the physical world. Moreover, we consider the \texttt{IA-Reality-call} \footnote{\url{http://networkrepository.com/ia-reality-call.php}} \cite{eagle2006reality}, \cite{nr} an undirected network that consists of human mobile phone call events between a small set of core users at the Massachusetts Institute of Technology (MIT) whom actually were assigned mobile phones for which all calls were collected. The network also contains edges between users that do not belong in the small set of users who called other individuals that were not actively monitored. Therefore, some sort of noise might exist in this network, since we do not have more information about the individuals outside of the small set of MIT users. Finally, we consider the directed \texttt{MathOverflow} \footnote{\url{http://snap.stanford.edu/data/sx-mathoverflow.html}} \cite{Paranjape:2017:MTN:3018661.3018731} network, where a node represents a user, and an edge indicates that user $u$ answered user's $v$ question on the website.

%\begin{table}
%	\centering
%	\scalebox{0.9}{
%	\begin{tabular}{l c c c c c} 
%		& \texttt{UC}  & \texttt{Digg} & \texttt{Dublin} & \texttt{Reality} & \texttt{MathOver}\\ 
%		\toprule
%		Assortativity Coef. & -0.187 & 0.004 & 0.708 & ?? & ??\\
%		Clustering Coef. & 	5.68 & 0.56 & 1.06 & ?? & ?? \\
%		Mean degree & 63.017 & 5.7653 & 75 & ?? & ?? \\
%		Size Largest SCC & 1294 & 6746 & 335 & ?? & 3256\\
%		Size of nodes & 1899  & 30398 & 10972 & ?? & 21688 \\
%		Size of Edges & 59835  & 87627 & 44517 & ?? & 107581 \\
%		Timespan & ? & ? & ? & ? & ?
%	\end{tabular}}
	
%\end{table}

%Some of the sets we came across are either outdated or incomplete to so degree which was unusable. The social network datasets however are well maintained by some of the major datasets distributors such as Stanford University, this due that most of the researchers prefer to use these datasets. During the experimentation phase we noticed that some of sets contain noise\footnote{Noise make the networks inconsistent to the real-world networks.} and so a clean-up is required before usage. ~~And, when same metrics are compared on different datasets their performance ranks are usually not consistent or even various greatly~~. 

Although there are many link prediction metrics and methods proposed, only very few works open their source codes. Re-implementing methods and formulas to calculate predictors is a time-consuming process. Only few public tools try to integrate these metrics and methods such as \texttt{linkpred}\footnote{\url{https://github.com/rafguns/linkpred}} and \texttt{LPmade} \footnote{\url{https://github.com/rlichtenwalter/LPmade}} \cite{lichtenwalter2011lpmade} which both have a handful of link prediction metrics. \texttt{LPmade} is a cross-platform software solution that provides multi-core link prediction and related tasks and analysis \cite{lichtenwalter2011lpmade}. It is written in \texttt{C++} and therefore it is suited for handling very large networks. Unfortunately, compilation issues and furthermore the lack of documentation and support prevented us from using the software. Hence, we used a \texttt{Python} library called \texttt{linkpred}. Unfortunately, due to the fact that it is entirely written \texttt{Python} it can prove to be very slow for handling large networks.

%It is a scalable library which implements the most commonly used unsupervised link prediction metrics, and furthermore supports automatic link prediction processes including prediction, evaluation, and network analysis

\bibliographystyle{abbrv}
\bibliography{snacspaper}  % sigproc.bib is the name of the Bibliography in this case

\end{document}
